import os
import json
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

TARGET_URL = "https://www.lifehacker.jp/regular/regular_book_to_read/"
DATA_DIR = "data"
KNOWN_URLS_PATH = f"{DATA_DIR}/known_urls.json"
EXCLUDED_URLS_PATH = f"{DATA_DIR}/excluded_urls.json"
ENV_FILE_PATH = ".env"
CARD_CONTAINER_SELECTOR = '[class*="articles_pArticles_Cards"]'

ANCHOR_SPONSORED_KEYWORDS = ["sponsored", "sponsored by"]


def load_env_file(path=ENV_FILE_PATH):
    """`.env` ã«è¨­å®šã•ã‚ŒãŸç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€"""
    if not os.path.exists(path):
        return

    with open(path, "r") as f:
        for line in f:
            stripped = line.strip()
            if not stripped or stripped.startswith("#"):
                continue
            if "=" not in stripped:
                continue
            key, value = stripped.split("=", 1)
            key = key.strip()
            value = value.strip().strip('"').strip("'")
            if key and key not in os.environ:
                os.environ[key] = value


load_env_file()


def ensure_data_dir():
    os.makedirs(DATA_DIR, exist_ok=True)


def load_json_list(path):
    if not os.path.exists(path):
        return []
    with open(path, "r") as f:
        return json.load(f)


def save_json_list(path, items):
    ensure_data_dir()
    with open(path, "w") as f:
        json.dump(sorted(set(items)), f, indent=2, ensure_ascii=False)


def normalize_url(href):
    if not href:
        return ""
    href = href.strip()
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return urljoin(TARGET_URL, href)
    return ""


def looks_like_review_link(href):
    if not href.startswith("https://www.lifehacker.jp/"):
        return False
    if any(
        segment in href for segment in ["/tag/", "/category/", "/author/", "/video/"]
    ):
        return False
    if "regular_book_to_read" in href or "/article/" in href:
        return True
    return False


def anchor_contains_sponsor(anchor):
    text = " ".join(anchor.stripped_strings).lower()
    return any(keyword in text for keyword in ANCHOR_SPONSORED_KEYWORDS)


def fetch_article_links():
    """æ›¸è©•ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLä¸€è¦§ã‚’å–å¾—"""
    try:
        res = requests.get(TARGET_URL, timeout=10)
        res.raise_for_status()
    except requests.RequestException as exc:
        print(f"[WARN] è¨˜äº‹ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return []
    soup = BeautifulSoup(res.text, "html.parser")

    card_links = set()
    # åºƒå‘Šãƒªãƒ³ã‚¯ã¯å¾Œæ®µã®JavaScriptãŒã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã§æŒ¿å…¥ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ãã€é™çš„ãªHTMLã‚’å–å¾—ã™ã‚‹ã ã‘ã§ã¯æ¤œå‡ºã§ããªã„ã€‚
    for card in soup.select(CARD_CONTAINER_SELECTOR):
        for anchor in card.find_all("a", href=True):
            href = normalize_url(anchor.get("href", ""))
            if not href:
                continue
            if anchor_contains_sponsor(anchor):
                continue
            if looks_like_review_link(href):
                card_links.add(href)

    if not card_links:
        reason = "Card ã‚»ãƒ¬ã‚¯ã‚¿ã«ä¸€è‡´ã™ã‚‹æ›¸è©•ãƒªãƒ³ã‚¯ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒšãƒ¼ã‚¸æ§‹é€ ãŒå¤‰ã‚ã£ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        notify_slack_error(reason)
        raise RuntimeError(reason)

    return sorted(card_links)


def post_to_slack(payload):
    webhook = os.environ.get("SLACK_BOOKREVIEW_WEBHOOK_URL")
    if not webhook:
        raise RuntimeError(
            "ç’°å¢ƒå¤‰æ•° SLACK_BOOKREVIEW_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        )

    res = requests.post(webhook, json=payload, timeout=5)
    try:
        res.raise_for_status()
    except requests.RequestException as exc:
        raise RuntimeError(f"Slack é€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}") from exc


def notify_slack(url):
    """Slack ã¸é€šçŸ¥"""
    message = {
        "text": f"ğŸ†• æ–°ã—ã„æ›¸è©•è¨˜äº‹ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸï¼\n{url}",
        "unfurl_links": True,
        "unfurl_media": True,
    }
    post_to_slack(message)


def notify_slack_error(reason):
    """Slack ã¸ã‚¨ãƒ©ãƒ¼é€šçŸ¥"""
    post_to_slack(
        {
            "text": f":warning: æ›¸è©•ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n{reason}",
            "unfurl_links": False,
            "unfurl_media": False,
        }
    )


def main():
    known_urls = set(load_json_list(KNOWN_URLS_PATH))
    current_urls = fetch_article_links()
    # print(f"Current URLs: {current_urls}")

    new_urls = [u for u in current_urls if u not in known_urls]

    print(f"Found {len(new_urls)} new URLs")

    for url in new_urls:
        print(f"Recording and notifying Slack: {url}")
        known_urls.add(url)
        notify_slack(url)

    save_json_list(KNOWN_URLS_PATH, list(known_urls))
    print("Done.")


if __name__ == "__main__":
    main()
